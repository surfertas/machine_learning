{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.3\n",
      "pandas 0.21.0\n",
      "sklearn 0.19.1\n",
      "scipy 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "\n",
    "for p in [np, pd, sklearn, scipy]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: To show efficacy of using features extracted from KNN as an input to a classifier. We should expact that the KNN features should contain better information to strengthen the signal and improve the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN summary:\n",
    "1. Non-parametric\n",
    "2. Used for both regression and classification.\n",
    "3. Computationally demanding as stores entire train data set.\n",
    "4. Needs to calculate similarity between input and all instances when predicting.\n",
    "5. Value k determines the number of neighbors to use in kNN model.\n",
    "6. Algorithm: Find k nearest neighors of the input, and use voting to determine class of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X = np.array(X)\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.1, random_state=10)\n",
    "X_train, X_valid, y_train, y_valid  = train_test_split(X_train, y_train, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_seed = 10\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n",
    "    # Starter class provided as an exercise in Advanced Machine Learning Course by\n",
    "    # National Research University Higher School of Economics\n",
    "    ''' This class implements KNN features extraction  '''\n",
    "    def __init__(self, n_jobs, k_list, metric, n_classes=None, n_neighbors=None, eps=1e-6):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.k_list = k_list\n",
    "        self.metric = metric\n",
    "        \n",
    "        if n_neighbors is None:\n",
    "            self.n_neighbors = max(k_list) \n",
    "        else:\n",
    "            self.n_neighbors = n_neighbors\n",
    "            \n",
    "        self.eps = eps        \n",
    "        self.n_classes_ = n_classes\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ''' Set's up the train set and self.NN object '''\n",
    "        # Create a NearestNeighbors (NN) object. \n",
    "        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n",
    "                                      metric=self.metric, \n",
    "                                      n_jobs=1, \n",
    "                                      algorithm='brute' if self.metric=='cosine' else 'auto')\n",
    "        self.NN.fit(X)\n",
    "        \n",
    "        # Store labels \n",
    "        self.y_train = y\n",
    "        \n",
    "        # Save how many classes we have\n",
    "        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n",
    "        \n",
    "    def predict(self, X): \n",
    "        ''' Produces KNN features for every object of a dataset X '''\n",
    "        if self.n_jobs == 1:\n",
    "            test_feats = []\n",
    "            for i in range(X.shape[0]):\n",
    "                test_feats.append(self.get_features_for_one(X[i:i+1]))\n",
    "        else:\n",
    "            p = Pool(self.n_jobs)\n",
    "            test_feats = p.map(self.get_features_for_one, map(lambda x: x.reshape(1, -1),X))\n",
    "            \n",
    "        return np.vstack(test_feats)\n",
    "    \n",
    "    def get_features_for_one(self, x):\n",
    "        ''' Computes KNN features for a single object `x` '''\n",
    "        NN_output = self.NN.kneighbors(x)\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores indices of the neighbors\n",
    "        neighs = NN_output[1][0]\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores distances to corresponding neighbors\n",
    "        neighs_dist = NN_output[0][0] \n",
    "\n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores labels of corresponding neighbors\n",
    "        neighs_y = self.y_train[neighs]\n",
    "\n",
    "        return_list = [] \n",
    "\n",
    "        ### 1. Fraction of objects of every class. ###\n",
    "        # It is basically a KNNÐ¡lassifiers predictions.\n",
    "        for k in self.k_list:\n",
    "            feats = np.array(neighs_y[:k])\n",
    "            # Specificy minlength to make sure assertion passes\n",
    "            feats = np.bincount(feats.astype('int'), minlength=self.n_classes)/k\n",
    "            assert np.sum(feats) == 1\n",
    "            assert len(feats) == self.n_classes\n",
    "            return_list += [feats]\n",
    "        \n",
    "        ### 2. Same label streak ###\n",
    "        # The largest number N, such that N nearest neighbors have the same label.\n",
    "        result = np.where(np.diff(neighs_y) != 0)\n",
    "        # Need to increment by 1 as np.diff reduces length\n",
    "        feats = np.array([result[0][0]+1] if result[0].size else [len(neighs_y)])\n",
    "        \n",
    "        assert len(feats) == 1\n",
    "        return_list += [feats]\n",
    "        \n",
    "        ### 3. Minimum distance to objects of each class ###\n",
    "        # Find the first instance of a class and take its distance as features.\n",
    "        #       \n",
    "        # If there are no neighboring objects of some classes, \n",
    "        # then set distance to that class to be 999.\n",
    "        feats = []\n",
    "        for c in range(self.n_classes):\n",
    "            idx = np.where(neighs_y == c)\n",
    "            # Use .size to check if np.where returns empty tuple\n",
    "            # TODO: Is this really the best way?\n",
    "            neighbors = 1 if idx[0].size else 0\n",
    "            dist = min(neighs_dist[idx]) if neighbors else 999\n",
    "            feats.append(dist)\n",
    "        feats = np.array(feats)    \n",
    "        assert len(feats) == self.n_classes\n",
    "        return_list += [feats]\n",
    "        \n",
    "        ### 4. Minimum *normalized* distance to objects of each class ###\n",
    "        # As 3. but we normalize (divide) the distances\n",
    "        # by the distance to the closest neighbor.\n",
    "               \n",
    "        # If there are no neighboring objects of some classes, \n",
    "        # Then set distance to that class to be 999.\n",
    "        feats = []\n",
    "        for c in range(self.n_classes):\n",
    "            idx = np.where(neighs_y == c)\n",
    "            neighbors = True if idx[0].size else False\n",
    "            dist = min(neighs_dist[idx]/(neighs_dist[0]+self.eps)) if neighbors else 999\n",
    "            feats.append(dist)\n",
    "        \n",
    "        feats = np.array(feats)\n",
    "        assert len(feats) == self.n_classes\n",
    "        return_list += [feats]\n",
    "        \n",
    "        ### 5. \n",
    "        # 5.1 Distance to Kth neighbor\n",
    "        #           \n",
    "        # 5.2 Distance to Kth neighbor normalized by distance to the first neighbor        \n",
    "        for k in self.k_list:\n",
    "            feat_51 = np.array(neighs_dist[k-1])\n",
    "            feat_52 = np.array(neighs_dist[k-1]/(neighs_dist[0] + self.eps))\n",
    "            feats = np.array([feat_51, feat_52])\n",
    "            return_list += [feats]\n",
    "            \n",
    "        ### 6. Mean distance to neighbors of each class for each K from `k_list` \n",
    "        # For each class select the neighbors of that class among K nearest neighbors \n",
    "        # and compute the average distance to those objects.\n",
    "        # If no objects of a certain class among K neighbors, set mean distance to 999\n",
    "        for k in self.k_list:\n",
    "            w = neighs_dist[:k]\n",
    "            count = np.bincount(neighs_y[:k], minlength=self.n_classes)\n",
    "            feats = np.bincount(neighs_y[:k],w, minlength=self.n_classes) / (count + self.eps)\n",
    "            feats[np.where(count == 0)] = 999\n",
    "            assert len(feats) == self.n_classes\n",
    "            return_list += [feats]\n",
    "        \n",
    "        # merge\n",
    "        knn_feats = np.hstack(return_list)\n",
    "        assert knn_feats.shape == (87,) or knn_feats.shape == (87, 1)\n",
    "        return knn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of K in KNN, starts with one \n",
    "k_list = [3, 8, 32]\n",
    "\n",
    "\n",
    "metric = 'minkowski'\n",
    "# Create instance of our KNN feature extractor\n",
    "NNF = NearestNeighborsFeats(n_jobs=2, k_list=k_list, metric=metric)\n",
    "\n",
    "# Fit on train set\n",
    "NNF.fit(X_train, y_train)\n",
    "\n",
    "# Get features for valid\n",
    "test_knn_feats = NNF.predict(X_test)\n",
    "\n",
    "# Dump the features to disk\n",
    "np.save('data/digits_knn_feats_%s_valid.npy' % metric , test_knn_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# We will use two metrics for KNN\n",
    "\n",
    "metric = 'minkowski'\n",
    "    \n",
    "# Set up splitting scheme, use StratifiedKFold\n",
    "# use skf_seed and n_splits defined above with shuffle=True\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=skf_seed)\n",
    "\n",
    "# Create instance of our KNN feature extractor\n",
    "# n_jobs can be larger than the number of cores\n",
    "NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n",
    "\n",
    "# Get KNN features using OOF use cross_val_predict with right parameters\n",
    "train_knn_feats = cross_val_predict(NNF, X_train,y_train,cv=skf)\n",
    "# Save the features\n",
    "np.save('data/knn_feats_%s_train.npy' % metric, train_knn_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train multi logistic classifier on original train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial',penalty='l1', solver='saga', tol=0.1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train multi logistic classifier on knn features generated from train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#print(metrics.accuracy_score(y_test, test_knn_feats))\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial',penalty='l1', solver='saga', tol=0.1)\n",
    "\n",
    "clf.fit(train_knn_feats, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "\n",
    "score = clf.score(test_knn_feats, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1131, 64)\n",
      "(1131, 87)\n",
      "(1131, 151)\n"
     ]
    }
   ],
   "source": [
    "combined_train = np.c_[X_train, train_knn_feats]\n",
    "combined_test = np.c_[X_test, test_knn_feats]\n",
    "print(X_train.shape)\n",
    "print(train_knn_feats.shape)\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train multi logistic classifier on knn features + original features found in train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial',penalty='l1', solver='saga', tol=0.1)\n",
    "\n",
    "clf.fit(combined_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "\n",
    "score = clf.score(combined_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
