{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=13)\n",
    "rc(\"text\", usetex=True)\n",
    "import daft\n",
    "%matplotlib inline\n",
    "#WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://en.wikipedia.org/wiki/Bayesian_network\n",
    "\n",
    "[2] http://daft-pgm.org/\n",
    "\n",
    "[3] Artificial Intelligence A Modern Approach (3rd Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayes Theorem\n",
    "2. Joint distribution that are intractable -> infuse domain knowledge to create graphical model.\n",
    "3. Bayes Net\n",
    "4. Computationally efficient to compute different probabilities\n",
    "5. Law of Total Probability\n",
    "6. Marginilization\n",
    "7. Example \n",
    "8. Markov Blanket\n",
    "9. D-separation\n",
    "10. finish with bayes net applied to classification problem (y, X, weights)\n",
    "\n",
    "\n",
    "Sampling\n",
    "1. gibbs sampling\n",
    "2. MH sampling\n",
    "3. Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is bayes theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand Bayes theorem, lets start with the joint distribution between randome variables \\\\(Y\\\\), which will designate as the cause, \\\\(X\\\\), as the effect. The variables are binary taking on values of either \\\\(0\\\\) or\\\\(1\\\\). \\\\[P(X,Y)\\\\]\n",
    "\n",
    "We can factor, or break apart, the joint in two different ways. \\\\[P(X,Y) = P(X|Y) P(Y) = P(Y|X)P(X)\\\\]\n",
    "\n",
    "Thus in order to determine, \\\\(P(Y|X)\\\\), the probability of the cause, given the effect, we can reorder the above, to obtain \\\\[\\frac{P(X|Y)P(Y)}{P(X)} = P(Y|X)\\\\]\n",
    "\n",
    "The above is often shown as \\\\[\\frac{(Likelihood)*(Prior)}{(Normalization)} = (Posterior)\\\\] \n",
    "\n",
    "Essentially, we are updating our prior belief of \\\\(Y\\\\) represented as \\\\(P(Y)\\\\) by the liklihood effect given the cause divided by some constant to arrive upon a new belief of \\\\(Y\\\\) given \\\\(X\\\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint distributions are hard to deal with..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often times the joint distribution is not practical to compute. Lets consider a set of random variables, consisting of 2 random binary variables, \\\\((X,Y)\\\\), again. We know that there are 4 possible outcomes, \\\\((0,0), (1,0), (0,1), (1,1)\\\\), thus we can easily compute the joint probability of \\\\(P(X=1, Y=0)\\\\) as \\\\(0.25\\\\). This was easy as we knew how many outcomes aligned with \\\\(X=1\\\\) and \\\\(Y=0\\\\) and also the set of all possible outcomes.\n",
    "\n",
    "Where is the difficult part? Well now consider a set of 20 random binary variables. To set of all possible outcomes is 2^20-1 or 1,048,575 combinations. Now consider random variables taking on an arbitrary number of values. You can quickly see how the \"all possible outcomes\" becomes difficult.\n",
    "\n",
    "Ok...but Bayes Theorem has doesnt contain the joint distribution. Well in fact it does, its just been hidden away. For example, consider the liklihood, \\\\(P(X|Y)\\\\), which breaks down to \\\\(\\frac{P(X,Y)}{P(Y)}\\\\). The joint has resurfaced.\n",
    "\n",
    "The point is that the joint distribution often represents an obstacle, which has resulted in the development of abstractions that help to address the intractability of some joint distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Net and Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical models are ways to address the computation load of some joint distributions by introducing domain experties in the form of cause and effect relationships. \n",
    "\n",
    "**\"A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).\"**- Wiki[1]\n",
    "\n",
    "**\"The first is to see the network as a representation of the joint probability distribution.  The second is to view\n",
    "it as an encoding of a collection of conditional independence statements.\"** - AIMA [2]\n",
    "\n",
    "Said simply its a graphical model defining conditional relationships between random variables, with connecting arrows (not bi-directional as its acyclic (not cyclical)) usually in the arrows flowing from causes to effects.\n",
    "\n",
    "Sticking with the example from Wikipedia, lets work with a small graphical model represented by a set of three random boolean variables, \\\\(s\\\\), \\\\(rain\\\\), \\\\(wet\\\\), where \\\\(s\\\\) stands for sprinkler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._axes.Axes at 0x7ff2d470dd68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAADCCAYAAAA4ukzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2RWwAEyK/B7Pn7d/tntW2MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2d470d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: Daft [2] is a great library to visualize graphical models! \n",
    "# Instantiate the PGM.\n",
    "pgm = daft.PGM([3.3, 3.05], origin=[0.1, 0.1])\n",
    "\n",
    "# Add nodes\n",
    "pgm.add_node(daft.Node(\"s\", r\"$s$\", 0.5, 2))\n",
    "pgm.add_node(daft.Node(\"rain\", r\"$rain$\", 1.5, 2))\n",
    "pgm.add_node(daft.Node(\"wet\", r\"$wet$\", 1, 1))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"rain\",\"s\")\n",
    "pgm.add_edge(\"s\", \"wet\")\n",
    "pgm.add_edge(\"rain\", \"wet\")\n",
    "\n",
    "pgm.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In words\n",
    "The probabilities that this graphical represents is as follows:  \\\\(P(s|rain)\\\\), the probability of a given state of the sprinkler given whether it rains or not,  \\\\(P(rain)\\\\), the probability of whether it rains or not, and \\\\(P(wet|s, rain)\\\\), the probability that its wet, given the state of the sprinkler and whether it rains or not.\n",
    "\n",
    "#### Conditional Probability Table(CPT)\n",
    "We can derive a CPT, conditional probability table, from the graphical model. This is where the domain knowledge comes into play as the CPT is typically provided.\n",
    "\n",
    "Lets assume the below CPT (I set the probabilities using some common sense):\n",
    "\n",
    "\\begin{array}{rr}\n",
    "P(s|r) \\\\ \\hline\n",
    "r & s &0.1  \\\\ \\hline\n",
    "r & \\neg s &0.9  \\\\ \\hline\n",
    "\\neg r &s &0.7  \\\\ \\hline\n",
    "\\neg r &\\neg s &0.3\\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{rr}\n",
    "P(r) \\\\ \\hline\n",
    "r  &0.3 \\\\ \\hline\n",
    "\\neg r  &0.7  \\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{rr}\n",
    "P(w|s,r) \\\\ \\hline\n",
    "r & s & w & 0.99 \\\\ \\hline\n",
    "r & s & \\neg w &0.01  \\\\ \\hline\n",
    "r & \\neg s &  w &0.98  \\\\ \\hline\n",
    "r & \\neg s & \\neg w &0.02 \\\\ \\hline\n",
    "\\neg r & s & w & 0.8 \\\\ \\hline\n",
    "\\neg r & s & \\neg w &0.2  \\\\ \\hline\n",
    "\\neg r & \\neg s &  w &0.05  \\\\ \\hline\n",
    "\\neg r & \\neg s & \\neg w &0.95  \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Probability that its not raining given the evidence that the sprinkler is on and its not wet.\n",
    "\\\\(P(\\neg r | s, \\neg w) =\\\\)\n",
    "\n",
    "\\\\(\\frac{P(\\neg r, s, \\neg w)}{P(s,\\neg w)}\\\\) [ Apply chain rule and reorder. ]\n",
    "\n",
    "\\\\(= \\frac{P(\\neg r, s, \\neg w)}{\\sum_r P(r=r,s,\\neg w)}\\\\) [ Marginalize the denominator for all values of \\\\(r\\\\). ]\n",
    "\n",
    "\\\\(= \\frac{P(\\neg r)P(s|\\neg r)P(\\neg w|s,\\neg r)}{\\sum_r P(r=r)P(s|r=r)P(\\neg w|s,r=r)}\\\\) [ Factor using the CPT. ]\n",
    "\n",
    "\\\\(= \\frac{P(\\neg r)P(s|\\neg r)P(\\neg w|s,\\neg r)}{\\sum_r P(r=r)P(s|r=r)P(\\neg w|s,r=r)}\\\\) [ Move terms not dependent on \\\\(r\\\\) out of the summation, in this case no simplification! ]\n",
    "\n",
    "\\\\(= \\frac{P(\\neg r)P(s|\\neg r)P(\\neg w|s,\\neg r)}{P(r)P(s|r)P(\\neg w|s,r) +  P(\\neg r)P(s|\\neg r)P(\\neg w|s,\\neg r)}\\\\) [ Expand. ]\n",
    "\n",
    "\\\\(= \\frac{(0.7)(0.7)(0.2)}{(0.3)(0.1)(0.01) +  (0.7)(0.7)(0.2)}\\\\) [ Plug in values based on CPT, the domain knowledge. ]\n",
    "\n",
    "\\\\(= 0.996948\\\\)\n",
    "\n",
    "This value seems reasonable, as we would expect that it was not raining if the sprinkler was on in addition if the grass was not wet.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Blanket\n",
    "\n",
    "Markov blanket for a node in a Bayesian Network is a set of nodes that consists of the nodes parents, children, and childrens parents.\n",
    "\n",
    "The Markov Blanket is a powerful tool in reducing the computational load. \n",
    "\n",
    "**\"The Markov blanket of a node contains all the variables that shield the node from the rest of the network. This means that the Markov blanket of a node is the only knowledge needed to predict the behavior of that node.\"** - Wiki\n",
    "\n",
    "Mathematically, \\\\[P(A|MB(A),B) = P(A|MB(A))\\\\]\n",
    "where \\\\(MB(A)\\\\) is the Markov Blanket of A\n",
    "\n",
    "Resources:\n",
    "1. https://en.wikipedia.org/wiki/Markov_blanket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-separation\n",
    "\n",
    "Rule 1: x and y are d-connected if there is an unblocked path between them. \n",
    "\n",
    "Rule 2: x and y are d-connected, conditioned on a set Z of nodes, if there is a collider-free path between x and y that traverses no member of Z. If no such path exists, we say that x and y are d-separated by Z, We also say then that every path between x and y is \"blocked\" by Z. \n",
    " \n",
    "Rule 3: If a collider is a member of the conditioning set Z, or has a descendant in Z, then it no longer blocks any path that traces this collider. \n",
    "\n",
    "*In summary, the introduction of knowledge **kills dependence** in the case of Rule 1 and Rule 2, while knowledge results in dependence in the case of Rule 3.*\n",
    "\n",
    "e.g. \\\\(P(C | A,B) = P(C | B)\\\\) is really asking if \\\\(C\\\\) is orthogonal to \\\\(A\\\\) given \\\\(B\\\\). If all paths are destroyed by this knowledge then \\\\(C\\\\) and \\\\(A\\\\) are independent and the equivalence holds. If a path is created, for example, if \\\\(B\\\\) is the point of collision on a path between \\\\(A\\\\) and \\\\(C\\\\), then \\\\(A\\\\) and \\\\(C\\\\) are not independent and the equivalence does not hold.\n",
    "\n",
    "Resources:\n",
    "1. http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html\n",
    "2. https://www.andrew.cmu.edu/user/scheines/tutor/d-sep.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
