{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Artificial Intelligence A Modern Approach (3rd Edition).pdf\n",
    "2. http://what-when-how.com/artificial-intelligence/a-comparison-of-cooling-schedules-for-simulated-annealing-artificial-intelligence/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill-climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psuedo Code\n",
    "def hill_climbing(problem):\n",
    "    current_state = problem.initial_state\n",
    "    while(True):\n",
    "        neighbor = max(current_state.neighbors)\n",
    "        if neighbor.value < current_state.value:\n",
    "            return current_state\n",
    "        else:\n",
    "            current_state = neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does not look beyond immediate neigbors.\n",
    "2. Result dependent on initial state and state space.\n",
    "3. The algorithm is prone to getting stuck at local maxima and is weak against \"ridges\".[1]\n",
    "4. Sometimes called greedy local search, as never makes \"downhill\" move, and in the context of exploration vs. exploitation, is only exploiting.\n",
    "5. To address local maxima issue, variants have been designed such as stochastic hill climbing, first-choice hill climbing, random-restart hill climbing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To explain simulated annealing,  we switch our point of view from hill climbing to\n",
    "gradient descent (i.e., minimizing cost) and imagine the task of getting a gradient descent ping-pong ball into the deepest crevice in a bumpy surface.  If we just let the ball roll, it will\n",
    "come to rest at a local minimum.  If we shake the surface, we can bounce the ball out of the\n",
    "local minimum.  The trick is to shake just hard enough to bounce the ball out of local min-\n",
    "ima but not hard enough to dislodge it from the global minimum.  The simulated-annealing\n",
    "solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the\n",
    "intensity of the shaking (i.e., lower the temperature).\" - AIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psuedo Code\n",
    "def simulated_annealing(problem, schedule):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        problem - the problem\n",
    "        schedule - a mapping from time t, to temperature T\n",
    "    Returns:\n",
    "        state - solution state.\n",
    "    \"\"\"\n",
    "    current_state = problem.initial_state\n",
    "    while (True):\n",
    "        T = schedule(t)\n",
    "        if T == 0:\n",
    "            return current_state\n",
    "        else:\n",
    "            next_state = random_select(current_state.neighbors)\n",
    "            delta_E = next_state.value - current_state.value\n",
    "            if delta_E > 0:\n",
    "                current_state = next_state\n",
    "            elif random.normal() > exp(delta_E/T): \n",
    "                # Exploration: Will take worse path with small probability.\n",
    "                current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tries to combine hill climbing (efficient but incomplete) with a random walk (inefficient but complete).\n",
    "2. Can be considered a stochastic version of hill climbing with some moves down, allows for some exploration.\n",
    "3. The \"cooling\" schedule is stated to be the most important factor in the algorithm. Some suggestions for annealing schedules are presented below. An indepth summary can be found at [2]\n",
    "\n",
    "Exponential multiplicative cooling: \\\\[T_k = T_0\\alpha^k \\ (0.8 \\leq \\alpha 0.9) \\\\]\n",
    "\n",
    "Logarithmical multiplicative cooling: \\\\[T_k = \\frac{T_0}{1+\\alpha k Log(1+k)} \\ (\\alpha > 1) \\\\]\n",
    "\n",
    "Linear multiplicative cooling: \\\\[T_k = \\frac{T_0}{1+\\alpha k} \\ (\\alpha > 0) \\\\]\n",
    "\n",
    "Quadratic multiplicative cooling: \\\\[T_k = \\frac{T_0}{1+\\alpha k^2} \\ (\\alpha > 0) \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_beam_search(problem):\n",
    "    (state_0, state_1,state_k) = problem.initial_k_states\n",
    "    states = (state_0, state_1,state_k)\n",
    "    while True:\n",
    "        for state in states:\n",
    "            if termination_condition(state.neighbors) == True:\n",
    "                return \"neighbor that meets condition\"\n",
    "            else:\n",
    "                all_states.append(state.neighbors)\n",
    "\n",
    "            states = select_k_max(all_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduces the expanded memory concept. As opposed to storing the last state, beam search stores k states.\n",
    "2. Generates sucessors for each of the k states, and takes the best k states if none represents termination.\n",
    "3. Can suffer from a lack of diversity among k-states as they can quickly become concentrated in a small region of the state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitness -> Selection -> Pairs -> Cross-Over -> Mutation\n",
    "1. The fitness function is used to evaluates the sample.\n",
    "2. A probability distribution of the samples based on fitness is used to calculate the likelihood of sample becoming a \"parent\".\n",
    "3. A split is selected randomly, and cross-over is used to create the offsping.\n",
    "4. GA mutation is used to infuse randomness to the offspring.\n",
    "5. Evolution is a natural dampening mechanism of randomness as resulting offspring output similar fitness functions and weights become more uniform.\n",
    "6. A variant of stochastic beam search, but not by asexual reproduction but by sexual reproduction, ie, the result of two parents. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example using the genetic algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a genetic algorithm in which individuals are represented using a 5-bit string of the form b1b2b3b4b5. An example of an individual is 00101, for which b1 = 0, b2 = 0, b3 = 1, b4 = 0, b5 = 1. The fitness function is defined over these individuals as follows:\n",
    "\n",
    " \n",
    "f(b1b2b3b4b5) = b1 + b2 + b3 + b4 + b5 + AND(b1, b2, b3, b4, b5)\n",
    "\n",
    " \n",
    "where\n",
    "\n",
    "\n",
    "AND(b1, b2, b3, b4, b5) = 1, if b1 = b2 = b3 = b4 = b5 = 1\n",
    "\n",
    "AND(b1, b2, b3, b4, b5) = 0, otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GeneticAlgo(object):\n",
    "    \n",
    "    def __init__(self, init_population, cross_over=3, mutate_idx=None):\n",
    "        self._parents = self._convert_string(init_population)\n",
    "        self._cross_over = cross_over\n",
    "        self._mutate_idx = mutate_idx\n",
    "        self._pop_size = len(self._parents)\n",
    "\n",
    "    def _convert_string(self, population):\n",
    "        parents = []\n",
    "        for individual in population:\n",
    "            bit_str = list(individual)\n",
    "            parent = [int(bit) for bit in bit_str]\n",
    "            parents.append(parent)\n",
    "        return parents\n",
    "\n",
    "    def update_population(self, population):\n",
    "        self._parents = population\n",
    "    \n",
    "    def get_parents(self, selection_probs):\n",
    "        parent1 = self._parents[np.random.choice(self._pop_size, 1, p=selection_probs)[0]]\n",
    "        parent2 = self._parents[np.random.choice(self._pop_size, 1, p=selection_probs)[0]]\n",
    "        return parent1, parent2\n",
    "\n",
    "    def get_child(self, parent1, parent2):\n",
    "        child = parent1[:self._cross_over] + parent2[self._cross_over:]\n",
    "        return child\n",
    "\n",
    "    def get_fit_values(self):\n",
    "        return [self.fitness_fn(x) for x in self._parents]\n",
    "\n",
    "    def mutate(self,child):\n",
    "        if self._mutate_idx is not None:\n",
    "            i = self._mutate_idx\n",
    "        else:\n",
    "            i = np.random.choice(5,1)[0]-1\n",
    "        child[i] = 1 ^ child[i]\n",
    "        return child\n",
    "    \n",
    "    @property\n",
    "    def pop_size(self):\n",
    "        return self._pop_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def fitness_fn(parent):\n",
    "        return sum(parent) + np.all(list(map(lambda bit: bit == 1, parent)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def selection_probs(fit_values):\n",
    "        return fit_values/np.sum(fit_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = ['00100', '11000', '01001', '10010', '00100']\n",
    "gen = GeneticAlgo(individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal solution is a 11111, with a fitness value of 6. As there is a small chance of mutation, we see the algorithm converge to a value shy of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9783\n",
      "4.89214\n",
      "4.637580000000001\n",
      "5.9927\n",
      "5.57824\n",
      "5.64364\n",
      "5.538360000000001\n",
      "4.7908800000000005\n",
      "5.1648\n",
      "5.9952\n"
     ]
    }
   ],
   "source": [
    "epoch_fitness = []\n",
    "for epoch in range(10):\n",
    "    fitness = []\n",
    "    for step in range(10000):\n",
    "        fit_values = gen.get_fit_values()\n",
    "        fitness.append(np.mean(fit_values))\n",
    "        selection_prob = gen.selection_probs(fit_values)\n",
    "        children = []\n",
    "        for i in range(gen.pop_size):\n",
    "            parent1, parent2 = gen.get_parents(selection_prob)\n",
    "            child = gen.get_child(parent1, parent2)\n",
    "            if np.random.uniform() < 0.001:\n",
    "                child = gen.mutate(child)\n",
    "                \n",
    "            children.append(child)\n",
    "        gen.update_population(children)\n",
    "    print(np.mean(fitness))\n",
    "    epoch_fitness.append(np.mean(fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
